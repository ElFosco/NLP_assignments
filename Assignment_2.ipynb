{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_assignments/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Fact checking, Neural Languange Inference (**NLI**)"
      ],
      "metadata": {
        "id": "3GM9DBN-Qz3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "kjzY5JamAmr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil  # file management\n",
        "import sys  # system\n",
        "import pandas as pd  # dataframe management\n",
        "import numpy as np  # data manipulation\n",
        "from tqdm import tqdm  # useful during debugging (progress bars)\n",
        "from typing import List, Callable, Dict  # typing\n",
        "import re  # regex\n",
        "import urllib.request  # download files\n",
        "import zipfile  # unzip files\n",
        "import gensim  # embeddings\n",
        "import gensim.downloader as gloader  # embeddings\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # one-hot encoding\n",
        "from matplotlib import pyplot as plt  # Plots\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  # Remove stopwords\n",
        "from nltk.stem import SnowballStemmer  # Stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.layers import Bidirectional, Dense, SimpleRNN,GlobalAveragePooling1D,Flatten, Concatenate, Add, Average, Dot, Dropout\n",
        "from keras.layers import concatenate, add, average, dot\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras import Input, Model\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# F1\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from functools import partial\n",
        "\n",
        "# Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import copy\n",
        "\n",
        "#split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ITDtNfau7Fo7",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:05:21.315102Z",
          "iopub.execute_input": "2021-12-16T18:05:21.315376Z",
          "iopub.status.idle": "2021-12-16T18:06:10.126460Z",
          "shell.execute_reply.started": "2021-12-16T18:05:21.315275Z",
          "shell.execute_reply": "2021-12-16T18:06:10.125679Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data"
      ],
      "metadata": {
        "id": "WSa612fnHwHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "metadata": {
        "id": "BspxZcRjW0NG",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:06:10.128307Z",
          "iopub.execute_input": "2021-12-16T18:06:10.128557Z",
          "iopub.status.idle": "2021-12-16T18:06:11.868593Z",
          "shell.execute_reply.started": "2021-12-16T18:06:10.128522Z",
          "shell.execute_reply": "2021-12-16T18:06:11.867794Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing of the Dataset"
      ],
      "metadata": {
        "id": "IVjyBn0wAqq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function used to preprocess the text\n",
        "def clean_text(text):\n",
        "\n",
        "  # delete multiple quotes\n",
        "  delete_multiple_quotes = \"''|``|\\.\\.\"\n",
        "  ris = re.sub(delete_multiple_quotes, '', text)\n",
        "\n",
        "  # get only the sentence, delete the number before it and the keywords after it\n",
        "  start_symbol = \"^[0-9]*\\\\t\"\n",
        "  end_symbol = \"( )?[\\.|\\?|\\!|\\,]( )?(\\\\t.*)?$\"\n",
        "  ris = re.sub(start_symbol, '', ris)\n",
        "  ris = re.sub(end_symbol, '', ris)\n",
        "\n",
        "  # convert the brackets into token, done for the claim string\n",
        "  ris = re.sub(\"\\(\", \" -LRB- \",ris)\n",
        "  ris = re.sub(\"\\)\", \" -RRB- \",ris)\n",
        "\n",
        "  # check if numbers are present between tokens LSB and RSB, if it's not the case delete the content\n",
        "  delete_content_lsb = \"-LRB-(.[^0-9]*)-RRB-\"\n",
        "  ris = re.sub(delete_content_lsb, '', ris)\n",
        "\n",
        "  # check if numbers are present in brackets, if it's not the case delete the content\n",
        "  delete_content_brackets = \"-LSB-(.[^0-9]*)-RSB-\"\n",
        "  ris = re.sub(delete_content_brackets, '', ris)\n",
        "\n",
        "  # delete brackets token\n",
        "  delete_brackets = \"-LRB-|-RRB-|-RSB-|-LSB-\"\n",
        "  ris = re.sub(delete_brackets, ' ', ris)\n",
        "\n",
        "  # deal with the &\n",
        "  ris = re.sub(\"\\&\", ' and ', ris)\n",
        "\n",
        "  # deal with the *\n",
        "  ris = re.sub(\"star * reach\", 'star*reach', ris)\n",
        "\n",
        "  # remove tokens that we are not interested in\n",
        "  remove_tokens = \"[\\-\\\"?!#`\\$]\"  # |[\\.] \" # added $ and \\. handled alone`\n",
        "  ris = re.sub(remove_tokens, ' ', ris)\n",
        "\n",
        "  ris.strip()\n",
        "\n",
        "  ris = \" \".join([LEMMATIZER.lemmatize(word) for word in ris.split()])\n",
        "\n",
        "  return ris.lower()"
      ],
      "metadata": {
        "id": "CclbQf-qH-Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemmer and lemmatizer definition\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "\n",
        "nltk.download('wordnet') \n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "YMKJBSGbIiX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading data for generating training dataset, using the preprocessing\n",
        "train_df = pd.read_csv('dataset/train_pairs.csv')\n",
        "train_df = train_df.drop(['Unnamed: 0'],axis=1)\n",
        "train_df['Evidence'] = train_df.apply(lambda row : clean_text(row['Evidence']), axis = 1)\n",
        "train_df['Claim'] = train_df.apply(lambda row : clean_text(row['Claim']), axis = 1)\n",
        "\n",
        "# reading data for generating test dataset, using the preprocessing\n",
        "test_df = pd.read_csv('dataset/test_pairs.csv')\n",
        "test_df = test_df.drop(['Unnamed: 0'],axis=1)\n",
        "test_df['Evidence'] = test_df.apply(lambda row : clean_text(row['Evidence']), axis = 1)\n",
        "test_df['Claim'] = test_df.apply(lambda row : clean_text(row['Claim']), axis = 1)\n",
        "\n",
        "# reading data for generating validation dataset, using the preprocessing\n",
        "valid_df = pd.read_csv('dataset/val_pairs.csv')\n",
        "valid_df = valid_df.drop(['Unnamed: 0'],axis=1)\n",
        "valid_df['Evidence'] = valid_df.apply(lambda row : clean_text(row['Evidence']), axis = 1)\n",
        "valid_df['Claim'] = valid_df.apply(lambda row : clean_text(row['Claim']), axis = 1)"
      ],
      "metadata": {
        "id": "PHntcbb2Gk86",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:06:11.870020Z",
          "iopub.execute_input": "2021-12-16T18:06:11.870453Z",
          "iopub.status.idle": "2021-12-16T18:06:48.587341Z",
          "shell.execute_reply.started": "2021-12-16T18:06:11.870414Z",
          "shell.execute_reply": "2021-12-16T18:06:48.586586Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop not relevant info\n",
        "X_train = train_df.drop(['Label','ID'], axis=1)\n",
        "X_val = valid_df.drop(['Label','ID'], axis=1)\n",
        "X_test = test_df.drop(['Label','ID'], axis=1)\n",
        "\n",
        "# generating y, for the label\n",
        "y_train = train_df['Label']\n",
        "y_val = valid_df['Label']\n",
        "y_test = test_df['Label']\n",
        "\n",
        "# converting the label, into 0 and 1, 0 for Refutes, 1 for Supports\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_val = label_encoder.transform(y_val)\n",
        "y_test = label_encoder.transform(y_test)\n"
      ],
      "metadata": {
        "id": "r-zg38glKHPQ",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:06:48.589382Z",
          "iopub.execute_input": "2021-12-16T18:06:48.589746Z",
          "iopub.status.idle": "2021-12-16T18:06:48.648088Z",
          "shell.execute_reply.started": "2021-12-16T18:06:48.589703Z",
          "shell.execute_reply": "2021-12-16T18:06:48.647498Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create GloVe embeddings "
      ],
      "metadata": {
        "id": "i8NEmJp1Ui6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "metadata": {
        "id": "6SgHagnSUlxq",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:06:48.654999Z",
          "iopub.execute_input": "2021-12-16T18:06:48.656070Z",
          "iopub.status.idle": "2021-12-16T18:06:48.665112Z",
          "shell.execute_reply.started": "2021-12-16T18:06:48.656032Z",
          "shell.execute_reply": "2021-12-16T18:06:48.664175Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_OOV_terms(embedding_vocabulary: List[str],\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    \n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "metadata": {
        "id": "Mff7TRr0V8hv",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:06:53.678913Z",
          "iopub.execute_input": "2021-12-16T18:06:53.679611Z",
          "iopub.status.idle": "2021-12-16T18:06:53.683599Z",
          "shell.execute_reply.started": "2021-12-16T18:06:53.679575Z",
          "shell.execute_reply": "2021-12-16T18:06:53.682979Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           vocab_size: int,\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def update_embedding_matrix(embedding_model: np.ndarray, \n",
        "                            embedding_dimension: int,\n",
        "                            word_to_idx: Dict[str, int],\n",
        "                            vocab_size: int,\n",
        "                            oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n",
        "\n",
        "    :param embedding_model: pre-trained embedding matrix\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[idx]\n",
        "        except (TypeError, IndexError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "ks3PUZeyFuYB",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:06:55.299998Z",
          "iopub.execute_input": "2021-12-16T18:06:55.300534Z",
          "iopub.status.idle": "2021-12-16T18:06:55.310205Z",
          "shell.execute_reply.started": "2021-12-16T18:06:55.300496Z",
          "shell.execute_reply": "2021-12-16T18:06:55.309420Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "kPAOAphzoPCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KerasTokenizer(object):\n",
        "    \"\"\"\n",
        "    A simple high-level wrapper for the Keras tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
        "                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n",
        "        if build_embedding_matrix:\n",
        "            assert embedding_model_type is not None\n",
        "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
        "\n",
        "        self.build_embedding_matrix = build_embedding_matrix\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.embedding_model_type = embedding_model_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.embedding_matrix = None\n",
        "        self.vocab = None\n",
        "\n",
        "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
        "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
        "\n",
        "        self.tokenizer_args = tokenizer_args\n",
        "\n",
        "    def build_vocab(self, data, **kwargs):\n",
        "        print('Fitting tokenizer...')\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        print('Fit completed!')\n",
        "\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "\n",
        "        if self.build_embedding_matrix:\n",
        "            if self.embedding_model is None:\n",
        "              print('Loading embedding model! It may take a while...')\n",
        "              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n",
        "                                                          embedding_dimension=self.embedding_dimension)\n",
        "            \n",
        "            print('Checking OOV terms in train...')\n",
        "            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n",
        "                                             word_listing=list(self.vocab.keys()))\n",
        "            \n",
        "            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n",
        "\n",
        "            print('Building the embedding matrix for train...')\n",
        "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
        "                                                           word_to_idx=self.vocab,\n",
        "                                                           vocab_size=len(self.vocab)+1,          \n",
        "                                                           embedding_dimension=self.embedding_dimension,\n",
        "                                                           oov_terms=self.oov_terms_train)\n",
        "            print('Done for train!')\n",
        "\n",
        "    def update_vocab(self, data, **kwargs):\n",
        "      self.tokenizer.fit_on_texts(data)\n",
        "      if self.build_embedding_matrix:\n",
        "        old_vocab = self.vocab\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "        print('Checking OOV terms...')\n",
        "        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n",
        "                                         word_listing=list(self.vocab.keys()))\n",
        "        \n",
        "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n",
        "\n",
        "        print('Building the embedding matrix...')\n",
        "        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n",
        "                                                       word_to_idx=self.vocab,\n",
        "                                                       vocab_size=len(self.vocab)+1,          \n",
        "                                                       embedding_dimension=self.embedding_dimension,\n",
        "                                                       oov_terms=self.oov_terms)\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            'build_embedding_matrix': self.build_embedding_matrix,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'embedding_model_type': self.embedding_model_type,\n",
        "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
        "            'embedding_model': self.embedding_model,\n",
        "            'vocab_size': len(self.vocab) + 1,\n",
        "        }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        if type(tokens) == str:\n",
        "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
        "        else:\n",
        "            return self.tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return self.tokenizer.sequences_to_texts(ids)"
      ],
      "metadata": {
        "id": "LHclnVYLXWZN",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:06:59.766374Z",
          "iopub.execute_input": "2021-12-16T18:06:59.766741Z",
          "iopub.status.idle": "2021-12-16T18:06:59.786975Z",
          "shell.execute_reply.started": "2021-12-16T18:06:59.766705Z",
          "shell.execute_reply": "2021-12-16T18:06:59.786288Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading embeddings"
      ],
      "metadata": {
        "id": "BGZ4NcIZoVwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 50\n",
        "embedding_model = load_embedding_model(model_type=\"glove\", \n",
        "                                       embedding_dimension=embedding_dimension)"
      ],
      "metadata": {
        "id": "1G9mrvhxYBL3",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:07:52.870101Z",
          "iopub.execute_input": "2021-12-16T18:07:52.871480Z",
          "iopub.status.idle": "2021-12-16T18:08:28.126816Z",
          "shell.execute_reply.started": "2021-12-16T18:07:52.871438Z",
          "shell.execute_reply": "2021-12-16T18:08:28.125906Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating tokenizer and Vocabulary"
      ],
      "metadata": {
        "id": "rDzHjGghoYpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_args = {\n",
        "    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n",
        "    'lower' : True,  # default\n",
        "    'filters' : '' \n",
        "}\n",
        "\n",
        "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
        "                           build_embedding_matrix=True,\n",
        "                           embedding_dimension=embedding_dimension,\n",
        "                           embedding_model_type=\"glove\", \n",
        "                           embedding_model=embedding_model)\n",
        "tokenizer.build_vocab(X_train[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_train[\"Claim\"])\n",
        "\n",
        "tokenizer_info = tokenizer.get_info()\n",
        "\n",
        "print('Tokenizer info: ', tokenizer_info)"
      ],
      "metadata": {
        "id": "rd7WJ8fOerEt",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:08:28.128198Z",
          "iopub.execute_input": "2021-12-16T18:08:28.128434Z",
          "iopub.status.idle": "2021-12-16T18:08:32.721025Z",
          "shell.execute_reply.started": "2021-12-16T18:08:28.128401Z",
          "shell.execute_reply": "2021-12-16T18:08:32.720287Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating tokenizer with validation and test"
      ],
      "metadata": {
        "id": "GA0kdGgcoczP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.update_vocab(X_val[\"Claim\"])\n",
        "tokenizer.update_vocab(X_test[\"Claim\"])\n",
        "tokenizer.update_vocab(X_val[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_test[\"Evidence\"])"
      ],
      "metadata": {
        "id": "b0jKa0qzNzce",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:08:32.722966Z",
          "iopub.execute_input": "2021-12-16T18:08:32.723470Z",
          "iopub.status.idle": "2021-12-16T18:08:33.654698Z",
          "shell.execute_reply.started": "2021-12-16T18:08:32.723432Z",
          "shell.execute_reply": "2021-12-16T18:08:33.653915Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = list(tokenizer.vocab.keys())\n",
        "a.sort()\n",
        "print(a)"
      ],
      "metadata": {
        "id": "rPHTtHKPkNPx",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:08:33.656015Z",
          "iopub.execute_input": "2021-12-16T18:08:33.657226Z",
          "iopub.status.idle": "2021-12-16T18:08:33.693343Z",
          "shell.execute_reply.started": "2021-12-16T18:08:33.657185Z",
          "shell.execute_reply": "2021-12-16T18:08:33.690543Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding for x and computation of max sequence length"
      ],
      "metadata": {
        "id": "8kxTuGGvotsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text(df, tokenizer, is_training=False, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Converts input text sequences using a given tokenizer\n",
        "\n",
        "    :param texts: either a list or numpy ndarray of strings\n",
        "    :tokenizer: an instantiated tokenizer\n",
        "    :is_training: whether input texts are from the training split or not\n",
        "    :max_seq_length: the max token sequence previously computed with\n",
        "    training texts.\n",
        "\n",
        "    :return\n",
        "        text_ids: a nested list on token indices\n",
        "        max_seq_length: the max token sequence previously computed with\n",
        "        training texts.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    text_ids_claim = tokenizer.convert_tokens_to_ids(df['Claim'])\n",
        "    text_ids_evidence = tokenizer.convert_tokens_to_ids(df['Evidence'])\n",
        "\n",
        "    # Padding\n",
        "    if is_training:\n",
        "        max_seq_length_claim = int(np.quantile([len(seq) for seq in text_ids_claim], 0.99))\n",
        "        max_seq_length_evidence = int(np.quantile([len(seq) for seq in text_ids_evidence], 0.99))\n",
        "\n",
        "        if max_seq_length_claim > max_seq_length_evidence:\n",
        "            max_seq_length = max_seq_length_claim\n",
        "        else:\n",
        "          max_seq_length = max_seq_length_evidence\n",
        "\n",
        "    else:\n",
        "        assert max_seq_length is not None\n",
        "\n",
        "    claims = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_claim]\n",
        "    claims = np.array([seq[:max_seq_length] for seq in claims])\n",
        "    \n",
        "    evidences = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_evidence]\n",
        "    evidences = np.array([seq[:max_seq_length] for seq in evidences])\n",
        "\n",
        "    return max_seq_length, np.array([claims, evidences])\n",
        "        \n",
        "\n",
        "max_seq_length, x_train = convert_text(X_train, tokenizer, True)\n",
        "print(\"Max token sequence: {}\".format(max_seq_length))\n",
        "print('X train shape: ', x_train.shape)\n",
        "\n",
        "_, x_val = convert_text(X_val, tokenizer, False, max_seq_length)\n",
        "print('X val shape: ', x_val.shape)\n",
        "\n",
        "_, x_test = convert_text(X_test, tokenizer, False, max_seq_length)\n",
        "print('X test shape: ', x_test.shape)"
      ],
      "metadata": {
        "id": "x1uKddQlsCRR",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:02.231373Z",
          "iopub.execute_input": "2021-12-16T18:09:02.232115Z",
          "iopub.status.idle": "2021-12-16T18:09:11.126347Z",
          "shell.execute_reply.started": "2021-12-16T18:09:02.232078Z",
          "shell.execute_reply": "2021-12-16T18:09:11.125553Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence embedding\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uhi2js_i6x0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vector_length = embedding_dimension"
      ],
      "metadata": {
        "id": "J7Ed3CA5FUkM",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:11.134208Z",
          "iopub.execute_input": "2021-12-16T18:09:11.134742Z",
          "iopub.status.idle": "2021-12-16T18:09:11.144810Z",
          "shell.execute_reply.started": "2021-12-16T18:09:11.134704Z",
          "shell.execute_reply": "2021-12-16T18:09:11.144128Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First Model"
      ],
      "metadata": {
        "id": "Wu-g7FQdJrEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode token sequences via a RNN and take the last state as the sentence embedding."
      ],
      "metadata": {
        "id": "C9bNgZt1JuOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def firstModel(embedding_vector_length, dim):\n",
        "\n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=True, \n",
        "                      mask_zero=True)(input)\n",
        "  # added l2 regularization due to overfitting\n",
        "  last_state = SimpleRNN(dim, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), \n",
        "                         bias_regularizer=l2(0.01), return_state=True)(x)\n",
        "  \n",
        "  RNN = Model(input, last_state, name=\"firstModel\")\n",
        "\n",
        "  return RNN"
      ],
      "metadata": {
        "id": "KVdlhBdF6sBn",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:11.146865Z",
          "iopub.execute_input": "2021-12-16T18:09:11.147416Z",
          "iopub.status.idle": "2021-12-16T18:09:11.154597Z",
          "shell.execute_reply.started": "2021-12-16T18:09:11.147380Z",
          "shell.execute_reply": "2021-12-16T18:09:11.153763Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example to check if it is working, no train done\n",
        "model_1 = firstModel(embedding_vector_length, 32)\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "model_1.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = model_1.predict(x_train[0][0].reshape(-1, len(x_train[0][0])))\n",
        "print(X_train.Claim[0])\n",
        "print(ris[1])"
      ],
      "metadata": {
        "id": "3Fc8SKdUZXHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Second Model"
      ],
      "metadata": {
        "id": "hBPosAWoJweN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode token sequences via a RNN and average all the output states."
      ],
      "metadata": {
        "id": "oFwjQ30lJ6e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def secondModel(embedding_vector_length, dim):\n",
        "  \n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=True, \n",
        "                      mask_zero=True)(input)\n",
        "  # added l2 regularization due to overfitting\n",
        "  states = SimpleRNN(dim, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), \n",
        "                     bias_regularizer=l2(0.01), return_sequences=True)(x)\n",
        "  output = GlobalAveragePooling1D()(states)\n",
        "  RNN = Model(input, output, name=\"secondModel\")\n",
        "  return RNN"
      ],
      "metadata": {
        "id": "leck2uGm6sBo",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:12.850173Z",
          "iopub.execute_input": "2021-12-16T18:09:12.850920Z",
          "iopub.status.idle": "2021-12-16T18:09:12.860025Z",
          "shell.execute_reply.started": "2021-12-16T18:09:12.850805Z",
          "shell.execute_reply": "2021-12-16T18:09:12.859187Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example to check if it is working, no train done\n",
        "model_2 = secondModel(embedding_vector_length,32)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "model_2.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = model_2.predict(x_train[0][0].reshape(-1, len(x_train[0][0])))\n",
        "print(X_train.Claim[0])\n",
        "print(ris[0])"
      ],
      "metadata": {
        "id": "u1M24UjOZZHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Third Model"
      ],
      "metadata": {
        "id": "eygbxvDbKNDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode token sequences via a simple MLP layer. "
      ],
      "metadata": {
        "id": "te7v_tgWKO8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def thirdModel(embedding_vector_length, dim):\n",
        "  \n",
        "  MLP = Sequential()\n",
        "  MLP.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=True, \n",
        "                      mask_zero=True))\n",
        "  MLP.add(Flatten())\n",
        "  MLP.add(Dense(256, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), input_shape=(embedding_vector_length*max_seq_length,), activation='relu'))\n",
        "  MLP.add(Dense(64, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n",
        "  MLP.add(Dense(dim, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='sigmoid'))\n",
        "\n",
        "  return MLP"
      ],
      "metadata": {
        "id": "X-ZntiyE6sBq",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:14.730095Z",
          "iopub.execute_input": "2021-12-16T18:09:14.730626Z",
          "iopub.status.idle": "2021-12-16T18:09:14.736303Z",
          "shell.execute_reply.started": "2021-12-16T18:09:14.730590Z",
          "shell.execute_reply": "2021-12-16T18:09:14.735455Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example to check if it is working, no train done\n",
        "model_3 = thirdModel(embedding_vector_length, 32)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "model_3.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = model_3.predict(x_train[0][0].reshape(-1, len(x_train[0][0])))\n",
        "print(X_train.Claim[0])\n",
        "print(ris[0])"
      ],
      "metadata": {
        "id": "TBShaJHsZtne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fourth Model"
      ],
      "metadata": {
        "id": "2WYtnxm6KhpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the sentence embedding as the mean of its token embeddings."
      ],
      "metadata": {
        "id": "K6oQ0DjBKkqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fourthModel(embedding_vector_length, dim=-1):  # dim used for convenience\n",
        "  \n",
        "  EMB = Sequential()\n",
        "  EMB.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=True, \n",
        "                      mask_zero=True))\n",
        "  EMB.add(GlobalAveragePooling1D())\n",
        "  return EMB"
      ],
      "metadata": {
        "id": "nR4MZomK6sBq",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:16.471814Z",
          "iopub.execute_input": "2021-12-16T18:09:16.472348Z",
          "iopub.status.idle": "2021-12-16T18:09:16.477531Z",
          "shell.execute_reply.started": "2021-12-16T18:09:16.472308Z",
          "shell.execute_reply": "2021-12-16T18:09:16.476799Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example to check if it is working, no train done\n",
        "model_4 = fourthModel(embedding_vector_length)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "model_4.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "ris = model_4.predict(x_train[0][0].reshape(-1, len(x_train[0][0])))\n",
        "print(X_train.Claim[0])\n",
        "print(ris[0])"
      ],
      "metadata": {
        "id": "10u4JfbsaJ_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merging multi-inputs"
      ],
      "metadata": {
        "id": "8W-aNp8OKvO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example to check if it is working, no train done\n",
        "emb_claim = model_1.predict(x_train[0][0].reshape(-1, len(x_train[0][0])))[1]\n",
        "emb_evidence = model_1.predict(x_train[1][0].reshape(-1, len(x_train[1][0])))[1]\n",
        "# 1 case\n",
        "first_emb = concatenate([emb_claim[0], emb_evidence[0]])\n",
        "print(first_emb)\n",
        "# 2 case\n",
        "second_emb = add([emb_claim[0], emb_evidence[0]])\n",
        "print(second_emb)\n",
        "# 3 case\n",
        "third_emb = average([emb_claim[0], emb_evidence[0]])\n",
        "print(third_emb)"
      ],
      "metadata": {
        "id": "oyGIMkYtdU9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity"
      ],
      "metadata": {
        "id": "9q8C3_i4EK3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fourth model taken as example\n",
        "model_4 = fourthModel(embedding_vector_length)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "model_4.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "emb_claim = model_4.predict(x_train[0][0].reshape(-1, len(x_train[0][0])))\n",
        "emb_evidence = model_4.predict(x_train[1][0].reshape(-1, len(x_train[1][0])))\n",
        "\n",
        "class_input = concatenate([emb_claim, emb_evidence])  # Concatenation as input\n",
        "print(\"Initial classifier input shape: \", class_input.shape)\n",
        "\n",
        "# cosine similaritty computed here\n",
        "cos_sim = dot([emb_claim, emb_evidence], axes=1, normalize=True)  # cos similarity\n",
        "print(\"\\nCosine similarity: \", cos_sim)\n",
        "\n",
        "class_input = concatenate([class_input, cos_sim])\n",
        "print(\"\\nFinal classifier input shape: \", class_input.shape)"
      ],
      "metadata": {
        "id": "pYPZN4gUEQTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "Qodi5wtE16yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Classifier(embed_model, model_type, type_merge, cosine_similarity,dense_size):\n",
        "  '''\n",
        "  Classifier, \n",
        "  Emdbed_model: model, model used for the sentence embedding\n",
        "  Model_type: string, the type of sentence embedding used \n",
        "  Type_merge: string, the type of merge used\n",
        "  Cosine_similarity: bool, indicates wether use it or not\n",
        "  Dense_size: int, the number of neuron used in the final Dense Layer\n",
        "  '''\n",
        "\n",
        "  input_c = Input(shape=(max_seq_length))\n",
        "  input_e = Input(shape=(max_seq_length))\n",
        "  embedding_c = embed_model(input_c)\n",
        "  embedding_e = embed_model(input_e)\n",
        "\n",
        "  # the first model returns the embeddings in a different position\n",
        "  if model_type == \"firstModel\":\n",
        "    embedding_c = embedding_c[1]\n",
        "    embedding_e = embedding_e[1]\n",
        "\n",
        "  # type of merge\n",
        "  if type_merge == \"concat\":\n",
        "      class_input = concatenate([embedding_c, embedding_e])\n",
        "  elif type_merge == \"sum\":\n",
        "      class_input = add([embedding_c, embedding_e])\n",
        "  elif type_merge == \"mean\":\n",
        "      class_input = average([embedding_c, embedding_e])\n",
        "\n",
        "  # using cosine_similarity\n",
        "  if cosine_similarity:\n",
        "      cos_sim = dot([embedding_c, embedding_e], axes=1, normalize=True)\n",
        "      class_input = concatenate([class_input, cos_sim])\n",
        "\n",
        "  x = Dropout(0.2)(class_input)\n",
        "  x = Dense(dense_size, activation=\"relu\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(x)\n",
        "  output = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  return Model([input_c, input_e], output, name=\"Classifier\")"
      ],
      "metadata": {
        "id": "fZjsHxzhd4N6",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:38.305666Z",
          "iopub.execute_input": "2021-12-16T18:09:38.305925Z",
          "iopub.status.idle": "2021-12-16T18:09:38.313633Z",
          "shell.execute_reply.started": "2021-12-16T18:09:38.305897Z",
          "shell.execute_reply": "2021-12-16T18:09:38.313008Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate models"
      ],
      "metadata": {
        "id": "NusqzqOCY8e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_f1(model, x_data, y_data):\n",
        "  predictions = model.predict(x_data)\n",
        "  predictions = np.round(predictions)\n",
        "\n",
        "  metrics = [\n",
        "             accuracy_score,\n",
        "             partial(f1_score, pos_label=1, average='macro')\n",
        "             ]\n",
        "  metric_names = [\n",
        "      \"accuracy\",\n",
        "      \"f1-score\"\n",
        "  ]\n",
        "  metric_info = evaluate_predictions(predictions=np.array(predictions),\n",
        "                                    y=np.array(y_data),\n",
        "                                    metrics=metrics,\n",
        "                                    metric_names=metric_names)\n",
        "  return metric_info"
      ],
      "metadata": {
        "id": "y1VJTN8vY_HS",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:11:51.065786Z",
          "iopub.execute_input": "2021-12-16T18:11:51.066544Z",
          "iopub.status.idle": "2021-12-16T18:11:51.076388Z",
          "shell.execute_reply.started": "2021-12-16T18:11:51.066503Z",
          "shell.execute_reply": "2021-12-16T18:11:51.072111Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "    \"\"\"\n",
        "    Evaluates given model predictions on a list of metric functions\n",
        "\n",
        "    :param predictions: model predictions in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "    :param metrics: list of metric functions\n",
        "    :param metric_names: list of metric names\n",
        "\n",
        "    :return\n",
        "        metric_info: dictionary containing metric values for each input metric\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(metrics) == len(metric_names)\n",
        "\n",
        "    metric_info = {}\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        metric_name = metric_names[i]\n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info"
      ],
      "metadata": {
        "id": "_B-pBtRqY-GB",
        "execution": {
          "iopub.status.busy": "2021-12-16T18:09:43.352392Z",
          "iopub.execute_input": "2021-12-16T18:09:43.353288Z",
          "iopub.status.idle": "2021-12-16T18:09:43.359258Z",
          "shell.execute_reply.started": "2021-12-16T18:09:43.353247Z",
          "shell.execute_reply": "2021-12-16T18:09:43.358279Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid-search"
      ],
      "metadata": {
        "id": "s4jhKvrLkfEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'firstModel': firstModel, 'secondModel': secondModel, \n",
        "          'thirdModel': thirdModel, 'fourthModel': fourthModel}\n",
        "parameters = {'epochs': range(10, 60, 10), \n",
        "              'batch_size':[128, 256, 512, 1024, 2048],\n",
        "              'dim': [32,64],\n",
        "              'merge_ops': ['mean', 'concat', 'sum'],  \n",
        "              'cos_sim': [True, False],                \n",
        "              'start_lr': [10**(-3),10**(-2),10**(-1)],\n",
        "              'dense_size' : [32,256]\n",
        "              }\n",
        "best_scores = {1: 0, 2: 0}\n",
        "best_params = {1: dict(), 2: dict()}\n",
        "\n",
        "for epochs in parameters['epochs']:\n",
        "  print(\"Epochs: \", epochs)\n",
        "  for start_lr in parameters['start_lr']:\n",
        "    print(\" Start Learning Rate: \", start_lr)\n",
        "    for batch_size in parameters['batch_size']:\n",
        "      print(\"  Batch Size: \", batch_size)\n",
        "      for dim in parameters['dim']:\n",
        "        print(\"   Dim: \", dim)\n",
        "        for model_name in models.keys():\n",
        "          print(\"    Model: \", model_name)\n",
        "          for merge_ops in parameters['merge_ops']:\n",
        "            print(\"     Merge: \", merge_ops)\n",
        "            for cos_sim in parameters['cos_sim']:\n",
        "              print(\"      Cosine Similarity: \", cos_sim)\n",
        "              for dense_size in parameters['dense_size']:\n",
        "                print(\"       Dense size: \", dense_size)\n",
        "                lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "                      start_lr,\n",
        "                      decay_steps=100000,\n",
        "                      decay_rate=0.96,\n",
        "                      staircase=True)\n",
        "                optim = Adam(learning_rate=lr_schedule)\n",
        "                embed_model = models[model_name](embedding_vector_length, dim)\n",
        "                model = Classifier(embed_model, model_name, merge_ops, \n",
        "                                  cosine_similarity=cos_sim,dense_size=dense_size)\n",
        "                model.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "                history = model.fit([x_train[0], x_train[1]], y_train, \n",
        "                          epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "                \n",
        "                scores = evaluate_f1(model, [x_val[0], x_val[1]], y_val)\n",
        "                print(\"     Scores: \", scores)\n",
        "                if scores['f1-score'] > best_scores[2]:\n",
        "                  if scores['f1-score'] > best_scores[1]:\n",
        "                    best_scores[2] = best_scores[1]\n",
        "                    best_scores[1] = scores['f1-score']\n",
        "                    best_params[2] = best_params[1]\n",
        "                    best_params[1] = {'epochs': epochs, 'batch_size': batch_size, \n",
        "                                      'dim': dim, 'start_lr': start_lr, \n",
        "                                      'model_name': model_name, \n",
        "                                      'merge_ops': merge_ops, 'cos_sim': cos_sim,\n",
        "                                      'dense_size':dense_size}\n",
        "                  else:\n",
        "                    best_scores[2] = scores['f1-score']\n",
        "                    best_params[2] = {'epochs': epochs, 'batch_size': batch_size, \n",
        "                                      'dim': dim, 'start_lr': start_lr, \n",
        "                                      'model_name': model_name, \n",
        "                                      'merge_ops': merge_ops, 'cos_sim': cos_sim,\n",
        "                                      'dense_size':dense_size}\n",
        "print(best_scores)\n",
        "print(best_params)\n",
        "                    "
      ],
      "metadata": {
        "id": "fCO359DYk8wQ",
        "execution": {
          "iopub.status.busy": "2021-12-16T19:50:16.653713Z",
          "iopub.execute_input": "2021-12-16T19:50:16.654001Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "wL8-eQO7BjJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'firstModel': firstModel, 'secondModel': secondModel, \n",
        "          'thirdModel': thirdModel, 'fourthModel': fourthModel}\n",
        "\n",
        "best_embed = best_params[1]['model_name']\n",
        "best_epochs = best_params[1]['epochs']\n",
        "best_batch_size = best_params[1]['batch_size']\n",
        "best_dim = best_params[1]['dim']\n",
        "best_start_lr = best_params[1]['start_lr']\n",
        "best_merge_ops = best_params[1]['merge_ops']\n",
        "best_cos_sim = best_params[1]['cos_sim']\n",
        "best_dense_size = best_params[1]['dense_size']"
      ],
      "metadata": {
        "id": "lLc6Zb96BnU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      best_start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "embed_model = models[best_embed](embedding_vector_length, best_dim)\n",
        "base_model = Classifier(embed_model, best_embed, best_merge_ops, \n",
        "                        cosine_similarity=best_cos_sim,dense_size=best_dense_size)\n",
        "base_model.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "history = base_model.fit(x=[x_train[0], x_train[1]], y=y_train, \n",
        "                         validation_data=([x_val[0], x_val[1]], y_val), \n",
        "                         epochs=best_epochs, batch_size=best_batch_size)"
      ],
      "metadata": {
        "id": "GF7mMXyMfeMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvH6c2z00T4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = base_model.predict([x_val[0], x_val[1]])\n",
        "print(classification_report(y_val, np.round(predictions)))"
      ],
      "metadata": {
        "id": "j4L2cshu1mYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.evaluate([x_test[0], x_test[1]], y_test)"
      ],
      "metadata": {
        "id": "_cQvMPwscFU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = base_model.predict([x_test[0], x_test[1]])\n",
        "print(classification_report(y_test, np.round(predictions)))"
      ],
      "metadata": {
        "id": "Wkho3lqr-F0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Majority voting"
      ],
      "metadata": {
        "id": "CDPfSUQpCbxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Multi input classification evaluation\n",
        "def inputClassificationEvaluation(y, predictions):\n",
        "  return classification_report(np.array(y), np.array(predictions), \n",
        "                               target_names=['refutes','supports'],\n",
        "                               labels=[0,1]);\n",
        "\n",
        "# B Claim verification evaluation\n",
        "def claim_verification_evaluation(X, y, predictions):\n",
        "  y_final=[]\n",
        "  predictions_final=[]\n",
        "  ris_label = -1\n",
        "  ris_predicted = -1\n",
        "\n",
        "  X['Label'] = y\n",
        "  X['Predicted'] = predictions\n",
        "  claims =  X.Claim.unique()\n",
        "\n",
        "  for el in claims:\n",
        "    # get every row with same Claim\n",
        "    rows = X.loc[X['Claim'] == el]\n",
        "    # get an array of the real label\n",
        "    label = np.array(rows.Label)\n",
        "    # get an array with the predictions\n",
        "    predicted = np.array(rows.Predicted)\n",
        "    # check the higher number of vote\n",
        "    if sum(label) >= label.size/2:\n",
        "      ris_label = 1\n",
        "    else:\n",
        "      ris_label = 0\n",
        "    if sum(predicted) >=  predicted.size/2:\n",
        "      ris_predicted = 1\n",
        "    else:\n",
        "      ris_predicted = 0\n",
        "    # append to the final result \n",
        "    y_final.append(ris_label)\n",
        "    predictions_final.append(ris_predicted)\n",
        "  return inputClassificationEvaluation(y_final,predictions_final)"
      ],
      "metadata": {
        "id": "SwSFFQDXApbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = base_model.predict([x_test[0], x_test[1]])\n",
        "print(claim_verification_evaluation(X_test, y_test, np.round(predictions)))"
      ],
      "metadata": {
        "id": "aCVTP1-_TmOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}