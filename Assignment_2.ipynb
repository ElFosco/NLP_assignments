{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_assignments/blob/carlo/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GM9DBN-Qz3k"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Due to**: 23/12/2021 (dd/mm/yyyy)\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Fact checking, Neural Languange Inference (**NLI**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO_-4CZeRCO7"
      },
      "source": [
        "# Intro\n",
        "\n",
        "This assignment is centred on a particular and emerging NLP task, formally known as **fact checking** (or fake checking). As AI techniques become more and more powerful, reaching amazing results, such as image and text generation, it is more than ever necessary to build tools able to distinguish what is real from what is fake.\n",
        "\n",
        "Here we focus on a small portion of the whole fact checking problem, which aims to determine whether a given statement (fact) conveys a trustworthy information or not. \n",
        "\n",
        "More precisely, given a set of evidences and a fact to verify, we would like our model to correctly predict whether the fact is true or fake.\n",
        "\n",
        "In particular, we will see:\n",
        "\n",
        "*   Dataset preparation (analysis and pre-processing)\n",
        "*   Problem formulation: multi-input binary classification\n",
        "*   Defining an evaluation method\n",
        "*   Simple sentence embedding\n",
        "*   Neural building blocks\n",
        "*   Neural architecture extension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGDwg78PS_uy"
      },
      "source": [
        "# The FEVER dataset\n",
        "\n",
        "First of all, we need to choose a dataset. In this assignment we will rely on the [FEVER dataset](https://fever.ai).\n",
        "\n",
        "The dataset is about facts taken from Wikipedia documents that have to be verified. In particular, facts could face manual modifications in order to define fake information or to give different formulations of the same concept.\n",
        "\n",
        "The dataset consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as ```Supported```, ```Refuted``` or ```NotEnoughInfo```. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oa5FpVpT7p4"
      },
      "source": [
        "## 2.1 Dataset structure\n",
        "\n",
        "Relevant data is divided into two file types. Information concerning the fact to verify, its verdict and associated supporting/opposing statements are stored in **.jsonl** format. In particular, each JSON element is a python dictionary with the following relevant fields:\n",
        "\n",
        "*    **ID**: ID associated to the fact to verify.\n",
        "\n",
        "*    **Verifiable**: whether the fact has been verified or not: ```VERIFIABLE``` or ```NOT VERIFIABLE```.\n",
        "    \n",
        "*    **Label**: the final verdict on the fact to verify: ```SUPPORTS```, ```REFUTES``` or ```NOT ENOUGH INFO```.\n",
        "    \n",
        "*    **Claim**: the fact to verify.\n",
        "    \n",
        "*    **Evidence**: a nested list of document IDs along with the sentence ID that is associated to the fact to verify. In particular, each list element is a tuple of four elements: the first two are internal annotator IDs that can be safely ignored; the third term is the document ID (called URL) and the last one is the sentence number (ID) in the pointed document to consider.\n",
        "\n",
        "**Some Examples**\n",
        "\n",
        "---\n",
        "\n",
        "**Verifiable**\n",
        "\n",
        "```\n",
        "{\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"New_Jersey_Turnpike\", 15]]]}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Not Verifiable**\n",
        "\n",
        "```\n",
        "{\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nex_8UM4VWuY"
      },
      "source": [
        "## 2.2 Some simplifications and pre-processing\n",
        "\n",
        "We are only interested in verifiable facts. Thus, we can filter out all non-verifiable claims.\n",
        "\n",
        "Additionally, the current dataset format does not contain all necessary information for our classification purposes. In particular, we need to download Wikipedia documents and replace reported evidence IDs with the corresponding text.\n",
        "\n",
        "Don't worry about that! We are providing you the already pre-processed dataset so that you can concentrate on the classification pipeline (pre-processing, model definition, evaluation and training).\n",
        "\n",
        "You can download the zip file containing all set splits (train, validation and test) of the FEVER dataset by clicking on this [link](https://drive.google.com/file/d/1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1/view?usp=sharing). Alternatively, run the below code cell to automatically download it on this notebook.\n",
        "\n",
        "**Note**: each dataset split is in .csv format. Feel free to inspect the whole dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITDtNfau7Fo7"
      },
      "source": [
        "import os, shutil  # file management\n",
        "import sys  # system\n",
        "import pandas as pd  # dataframe management\n",
        "import numpy as np  # data manipulation\n",
        "from tqdm import tqdm  # useful during debugging (progress bars)\n",
        "from typing import List, Callable, Dict  # typing\n",
        "import re  # regex\n",
        "import urllib.request  # download files\n",
        "import zipfile  # unzip files\n",
        "import gensim  # embeddings\n",
        "import gensim.downloader as gloader  # embeddings\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # one-hot encoding\n",
        "from matplotlib import pyplot as plt  # Plots\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  # Remove stopwords\n",
        "from nltk.stem import SnowballStemmer  # Stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.layers import Bidirectional, Dense, SimpleRNN,GlobalAveragePooling1D,Flatten, Concatenate, Add, Average, Dot\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras import Input, Model\n",
        "\n",
        "# F1\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from functools import partial\n",
        "\n",
        "# Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import copy\n",
        "\n",
        "#split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BspxZcRjW0NG"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHntcbb2Gk86"
      },
      "source": [
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "\n",
        "nltk.download('wordnet') \n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "  #delete multiple quotes\n",
        "  delete_multiple_quotes = \"''|``|\\.\\.\"\n",
        "  ris = re.sub(delete_multiple_quotes, '', text)\n",
        "\n",
        "  #get only the sentence, delete the number before it and the keywords after it\n",
        "  start_symbol = \"^[0-9]*\\\\t\"\n",
        "  end_symbol = \"( )?[\\.|\\?|\\!|\\,]( )?(\\\\t.*)?$\"\n",
        "  ris = re.sub(start_symbol, '', ris)\n",
        "  ris = re.sub(end_symbol, '', ris)\n",
        "\n",
        "  #convert the brackets into token, done for the claim string\n",
        "  #ris = re.sub(\"\\(\", \" -LRB- \",ris)\n",
        "  #ris = re.sub(\"\\)\", \" -RRB- \",ris)\n",
        "\n",
        "  ris = re.sub(\" -LRB- \", \" ( \",ris)\n",
        "  ris = re.sub(\" -RRB- \", \" ) \",ris)\n",
        "  ris = re.sub(\" -LSB- \", \" [ \",ris)\n",
        "  ris = re.sub(\" -RSB- \", \" ] \",ris)\n",
        "\n",
        "  #check if numbers are present between tokens LSB and RSB, if it's not the case delete the content\n",
        "  #delete_content_lsb = \"-LRB-(.[^0-9]*)-RRB-\"\n",
        "  #ris = re.sub(delete_content_lsb, '', ris)\n",
        "\n",
        "  #check if numbers are present in brackets, if it's not the case delete the content\n",
        "  #delete_content_brackets = \"-LSB-(.[^0-9]*)-RSB-\"\n",
        "  #ris = re.sub(delete_content_brackets, '', ris)\n",
        "\n",
        "  #delete brackets token\n",
        "  #delete_brackets = \"-LRB-|-RRB-|-RSB-|-LSB-\"\n",
        "  #ris = re.sub(delete_brackets, ' ', ris)\n",
        "\n",
        "  #deal with the &\n",
        "  ris = re.sub(\"\\&\", ' and ', ris)\n",
        "\n",
        "  #deal with the *\n",
        "  ris = re.sub(\"star * reach\", 'star*reach', ris)\n",
        "\n",
        "  #remove tokens that we are not interested in\n",
        "  remove_tokens = \"[\\-\\\"?!#`\\$]\"  # |[\\.] \" # added $ and \\. handled alone`\n",
        "  ris = re.sub(remove_tokens, ' ', ris)\n",
        "\n",
        "  #delete additional spaces and last space\n",
        "  #remove_spaces = \"[ ]+\"\n",
        "  #remove_last_spaces= \" $\"\n",
        "  #ris = re.sub(remove_spaces, ' ', ris)\n",
        "  #ris = re.sub(remove_last_spaces, '', ris)\n",
        "\n",
        "  #ris = ' '.join([x for x in ris.split() if x and x not in STOPWORDS])\n",
        "  ris.strip()\n",
        "\n",
        "  ris = \" \".join([LEMMATIZER.lemmatize(word) for word in ris.split()])\n",
        "\n",
        "  #handle ' (?)\n",
        "\n",
        "  return ris.lower()\n",
        "\n",
        "id=1294\n",
        "df = pd.read_csv('dataset/test_pairs.csv')\n",
        "df = df.drop(['Unnamed: 0'],axis=1)\n",
        "print(df.iloc[id]['Evidence'])\n",
        "df['Evidence'] = df.apply(lambda row : clean_text(row['Evidence']), axis = 1)\n",
        "df['Claim'] = df.apply(lambda row : clean_text(row['Claim']), axis = 1)\n",
        "print(df.iloc[id]['Evidence'])\n",
        "print(df.iloc[id]['Claim'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-zg38glKHPQ"
      },
      "source": [
        "X = df.drop(['Label','ID'],axis=1)\n",
        "y = df['Label']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.66, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbH_8errW5MH"
      },
      "source": [
        "# Classification dataset\n",
        "\n",
        "At this point, you should have a reay-to-go dataset! Note that the dataset format changed as well! In particular, we split the evidence set associated to each claim, in order to build `(claim, evidence)` pairs. The classification label is propagated as well.\n",
        "\n",
        "We'll motivate this decision in the next section!\n",
        "\n",
        "Just for clarity, here's an example of the pre-processed dataset:\n",
        "\n",
        "---\n",
        "\n",
        "**Claim**: \"Wentworth Miller is yet to make his screenwriting debut.\"\n",
        "\n",
        "**Evidence**: \"2\tHe made his screenwriting debut with the 2013 thriller film Stoker .\tStoker\tStoker (film)\"\n",
        "\n",
        "**Label**: Refutes\n",
        "\n",
        "---\n",
        "\n",
        "[**Note**]: The dataset requires some text cleaning as you may have noticed!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH8hIK21Xrl0"
      },
      "source": [
        "# Problem formulation\n",
        "\n",
        "As mentioned at the beginning of the assignment, we are going to formulate the fact checking problem as a binary classification task.\n",
        "\n",
        "In particular, each dataset sample is comprised of:\n",
        "\n",
        "*     A claim to verify\n",
        "*     A set of semantically related statements (evidence set)\n",
        "*     Fact checking label: either evidences support or refute the claim.\n",
        "\n",
        "Handling the evidence set from the point of view of neural models may imply some additional complexity: if the evidence set is comprised of several sentences we might incur in memory problems.\n",
        "\n",
        "To this end, we further simplify the problem by building (claim, evidence) pairs. The fact checking label is propagated as well.\n",
        "\n",
        "Example:\n",
        "\n",
        "     Claim: c1 \n",
        "     Evidence set: [e1, e2, e3]\n",
        "     Label: S (support)\n",
        "\n",
        "--->\n",
        "\n",
        "    (c1, e1, S),\n",
        "    (c1, e2, S),\n",
        "    (c1, e3, S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E46flIz_zQy-"
      },
      "source": [
        "## 4.1 Schema\n",
        "\n",
        "The overall binary classification problem is summed up by the following (simplified) schema\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Wm_YBnFwgJtxcWEBpPbTBEVkpKaL08Jp)\n",
        "\n",
        "Don't worry too much about the **Encoding** block for now. We'll give you some simple guidelines about its definition. For the moment, stick to the binary classification task definition where, in this case, we have 2 inputs: the claim to verify and one of its associated evidences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsiTV-NVdgsF"
      },
      "source": [
        "# Architecture Guidelines\n",
        "\n",
        "There are many neural architectures that follow the above schema. To avoid phenomena like the writer's block, in this section we are going to give you some implementation guidelines.\n",
        "\n",
        "In particular, we would like you to test some implementations so that you explore basic approaches (neural baselines) and use them as building blocks for possible extensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJBQm47fe7iE"
      },
      "source": [
        "## 5.1 Handling multiple inputs\n",
        "\n",
        "The first thing to notice is that we are in a multi-input scenario. In particular, each sample is comprised of a fact and its asssociated evidence statement.\n",
        "\n",
        "Each of these input is encoded as a sequence of tokens. In particular, we will have the following input matrices:\n",
        "\n",
        "*    Claim: `[batch_size, max_tokens]`\n",
        "*    Evidence: `[batch_size, max_tokens]`\n",
        "\n",
        "Moreover, after the embedding layer, we'll have:\n",
        "\n",
        "*    Claim: `[batch_size, max_tokens, embedding_dim]`\n",
        "*    Evidence: `[batch_size, max_tokens, embedding_dim]`\n",
        "\n",
        "But, we would like to have a 2D input to our classifier, since we have to give an answer at pair level. Therefore, for each sample, we would expect the following input shape to our classification block:\n",
        "\n",
        "*   Classification input shape: `[batch_size, dim]`\n",
        "\n",
        "**How to do that?**\n",
        "\n",
        "We inherently need to reduce the token sequence to a single representation. This operation is formally known as **sentence embedding**. Indeed, we are trying to compress the information of a whole sequence into a single embedding vector.\n",
        "\n",
        "Here are some simple solutions that we ask you to try out:\n",
        "\n",
        "1.   Encode token sequences via a RNN and take the last state as the sentence embedding.\n",
        "\n",
        "2.  Encode token sequences via a RNN and average all the output states.\n",
        "\n",
        "3.  Encode token sequences via a simple MLP layer. In particular, if your input is a `[batch_size, max_tokens, embedding_dim]` tensor, the matrix multiplication works on the **max_tokens** dimension, resulting in a `[batch_size, embedding_dim]` 2D matrix. Alternatively, you can reshape the 3D input tensor from `[batch_size, max_tokens, embedding_dim]` to `[batch_size, max_tokens * embedding_dim]` and then apply the MLP layer.\n",
        "\n",
        "4.   Compute the sentence embedding as the mean of its token embeddings (**bag of vectors**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8NEmJp1Ui6z"
      },
      "source": [
        "## Create GloVe embeddings (keep attention to the size, maybe we have to change it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SgHagnSUlxq"
      },
      "source": [
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mff7TRr0V8hv"
      },
      "source": [
        "def check_OOV_terms(embedding_vocabulary: List[str],\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    \n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks3PUZeyFuYB"
      },
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           vocab_size: int,\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def update_embedding_matrix(embedding_model: np.ndarray, \n",
        "                            embedding_dimension: int,\n",
        "                            word_to_idx: Dict[str, int],\n",
        "                            vocab_size: int,\n",
        "                            oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n",
        "\n",
        "    :param embedding_model: pre-trained embedding matrix\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[idx]\n",
        "        except (TypeError, IndexError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPAOAphzoPCY"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHclnVYLXWZN"
      },
      "source": [
        "class KerasTokenizer(object):\n",
        "    \"\"\"\n",
        "    A simple high-level wrapper for the Keras tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
        "                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n",
        "        if build_embedding_matrix:\n",
        "            assert embedding_model_type is not None\n",
        "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
        "\n",
        "        self.build_embedding_matrix = build_embedding_matrix\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.embedding_model_type = embedding_model_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.embedding_matrix = None\n",
        "        self.vocab = None\n",
        "\n",
        "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
        "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
        "\n",
        "        self.tokenizer_args = tokenizer_args\n",
        "\n",
        "    def build_vocab(self, data, **kwargs):\n",
        "        print('Fitting tokenizer...')\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        print('Fit completed!')\n",
        "\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "\n",
        "        if self.build_embedding_matrix:\n",
        "            if self.embedding_model is None:\n",
        "              print('Loading embedding model! It may take a while...')\n",
        "              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n",
        "                                                          embedding_dimension=self.embedding_dimension)\n",
        "            \n",
        "            print('Checking OOV terms in train...')\n",
        "            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n",
        "                                             word_listing=list(self.vocab.keys()))\n",
        "            \n",
        "            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n",
        "\n",
        "            print('Building the embedding matrix for train...')\n",
        "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
        "                                                           word_to_idx=self.vocab,\n",
        "                                                           vocab_size=len(self.vocab)+1,          \n",
        "                                                           embedding_dimension=self.embedding_dimension,\n",
        "                                                           oov_terms=self.oov_terms_train)\n",
        "            print('Done for train!')\n",
        "\n",
        "    def update_vocab(self, data, **kwargs):\n",
        "      self.tokenizer.fit_on_texts(data)\n",
        "      if self.build_embedding_matrix:\n",
        "        old_vocab = self.vocab\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "        print('Checking OOV terms...')\n",
        "        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n",
        "                                         word_listing=list(self.vocab.keys()))\n",
        "        \n",
        "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n",
        "\n",
        "        print('Building the embedding matrix...')\n",
        "        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n",
        "                                                       word_to_idx=self.vocab,\n",
        "                                                       vocab_size=len(self.vocab)+1,          \n",
        "                                                       embedding_dimension=self.embedding_dimension,\n",
        "                                                       oov_terms=self.oov_terms)\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            'build_embedding_matrix': self.build_embedding_matrix,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'embedding_model_type': self.embedding_model_type,\n",
        "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
        "            'embedding_model': self.embedding_model,\n",
        "            'vocab_size': len(self.vocab) + 1,\n",
        "        }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        if type(tokens) == str:\n",
        "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
        "        else:\n",
        "            return self.tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return self.tokenizer.sequences_to_texts(ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGZ4NcIZoVwS"
      },
      "source": [
        "### Downloading embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G9mrvhxYBL3"
      },
      "source": [
        "embedding_dimension = 50\n",
        "embedding_model = load_embedding_model(model_type=\"glove\", \n",
        "                                       embedding_dimension=embedding_dimension)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDzHjGghoYpP"
      },
      "source": [
        "### Creating tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd7WJ8fOerEt"
      },
      "source": [
        "tokenizer_args = {\n",
        "    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n",
        "    'lower' : True,  # default\n",
        "    'filters' : '' \n",
        "}\n",
        "\n",
        "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
        "                           build_embedding_matrix=True,\n",
        "                           embedding_dimension=embedding_dimension,\n",
        "                           embedding_model_type=\"glove\", \n",
        "                           embedding_model=embedding_model)\n",
        "tokenizer.build_vocab(X_train[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_train[\"Claim\"])\n",
        "\n",
        "tokenizer_info = tokenizer.get_info()\n",
        "\n",
        "print('Tokenizer info: ', tokenizer_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA0kdGgcoczP"
      },
      "source": [
        "### Updating tokenizer with validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0jKa0qzNzce"
      },
      "source": [
        "tokenizer.update_vocab(X_val[\"Claim\"])\n",
        "tokenizer.update_vocab(X_test[\"Claim\"])\n",
        "tokenizer.update_vocab(X_val[\"Evidence\"])\n",
        "tokenizer.update_vocab(X_test[\"Evidence\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPHTtHKPkNPx"
      },
      "source": [
        "a = list(tokenizer.vocab.keys())\n",
        "a.sort()\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kxTuGGvotsf"
      },
      "source": [
        "### Padding for x and computation of max sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1uKddQlsCRR"
      },
      "source": [
        "def convert_text(df, tokenizer, is_training=False, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Converts input text sequences using a given tokenizer\n",
        "\n",
        "    :param texts: either a list or numpy ndarray of strings\n",
        "    :tokenizer: an instantiated tokenizer\n",
        "    :is_training: whether input texts are from the training split or not\n",
        "    :max_seq_length: the max token sequence previously computed with\n",
        "    training texts.\n",
        "\n",
        "    :return\n",
        "        text_ids: a nested list on token indices\n",
        "        max_seq_length: the max token sequence previously computed with\n",
        "        training texts.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    text_ids_claim = tokenizer.convert_tokens_to_ids(df['Claim'])\n",
        "    text_ids_evidence = tokenizer.convert_tokens_to_ids(df['Evidence'])\n",
        "\n",
        "    # Padding\n",
        "    if is_training:\n",
        "        max_seq_length_claim = int(np.quantile([len(seq) for seq in text_ids_claim], 0.99))\n",
        "        max_seq_length_evidence = int(np.quantile([len(seq) for seq in text_ids_evidence], 0.99))\n",
        "\n",
        "        if max_seq_length_claim > max_seq_length_evidence:\n",
        "            max_seq_length = max_seq_length_claim\n",
        "        else:\n",
        "          max_seq_length = max_seq_length_evidence\n",
        "\n",
        "    else:\n",
        "        assert max_seq_length is not None\n",
        "\n",
        "    claims = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_claim]\n",
        "    claims = np.array([seq[:max_seq_length] for seq in claims])\n",
        "    \n",
        "    evidences = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids_evidence]\n",
        "    evidences = np.array([seq[:max_seq_length] for seq in evidences])\n",
        "\n",
        "    return max_seq_length, np.array([claims, evidences])\n",
        "        \n",
        "\n",
        "max_seq_length, x_train = convert_text(X_train, tokenizer, True)\n",
        "print(\"Max token sequence: {}\".format(max_seq_length))\n",
        "print('X train shape: ', x_train.shape)\n",
        "\n",
        "_, x_val = convert_text(X_val, tokenizer, False, max_seq_length)\n",
        "print('X val shape: ', x_val.shape)\n",
        "\n",
        "_, x_test = convert_text(X_test, tokenizer, False, max_seq_length)\n",
        "print('X test shape: ', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhi2js_i6x0Z"
      },
      "source": [
        "## Sentence embedding\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Ed3CA5FUkM"
      },
      "source": [
        "embedding_vector_length = embedding_dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyF5m7TP5nD7"
      },
      "source": [
        "def firstModel(embedding_vector_length,dim, start_lr=0.001):\n",
        "\n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=True, \n",
        "                      mask_zero=True)(input)\n",
        "  last_state = SimpleRNN(dim,return_state=True)(x)   #can we add a Bidirectional layer??\n",
        "  \n",
        "  RNN = Model(input, last_state, name=\"firstModel\")\n",
        "  return RNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEfIzk7vFNcd"
      },
      "source": [
        "def secondModel(embedding_vector_length,dim, start_lr=0.001):\n",
        "  \n",
        "  input = Input(shape=(max_seq_length))\n",
        "  x = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True)(input)\n",
        "  states = SimpleRNN(dim,return_sequences=True)(x)   #can we add a Bidirectional layer??\n",
        "  output = GlobalAveragePooling1D()(states)\n",
        "  RNN = Model(input, output, name=\"secondModel\")\n",
        "  \n",
        "  \n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  RNN.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "  return RNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7u666kwGsqG"
      },
      "source": [
        "def thirdModel(embedding_vector_length, dim, start_lr=0.001):\n",
        "  \n",
        "  MLP = Sequential()\n",
        "  MLP.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True))\n",
        "  MLP.add(Flatten())\n",
        "  MLP.add(Dense(350, input_shape=(embedding_vector_length*max_seq_length,), activation='relu'))\n",
        "  MLP.add(Dense(50, activation='relu'))\n",
        "  MLP.add(Dense(dim, activation='sigmoid'))\n",
        "  \n",
        "  \n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  MLP.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "\n",
        "  return MLP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSJckeXRHMNs"
      },
      "source": [
        "def fourthModel(embedding_vector_length, start_lr=0.001):\n",
        "  \n",
        "  EMB = Sequential()\n",
        "  EMB.add(Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                      input_length=max_seq_length, \n",
        "                      trainable=False, \n",
        "                      mask_zero=True))  # do not train embeddings, but we can do it\n",
        "  EMB.add(GlobalAveragePooling1D())\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      start_lr,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "  \n",
        "  optim = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  EMB.compile(loss='categorical_crossentropy', optimizer=optim, \n",
        "                  metrics=['accuracy']) #to check\n",
        "  return EMB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gypl5z5ElJo1"
      },
      "source": [
        "## 5.2 Merging multi-inputs\n",
        "\n",
        "At this point, we have to think about **how** we should merge evidence and claim sentence embeddings.\n",
        "\n",
        "For simplicity, we stick to simple merging strategies:\n",
        "\n",
        "*     **Concatenation**: define the classification input as the concatenation of evidence and claim sentence embeddings\n",
        "\n",
        "*     **Sum**: define the classification input as the sum of evidence and claim sentence embeddings\n",
        "\n",
        "*     **Mean**: define the classification input as the mean of evidence and claim sentence embeddings\n",
        "\n",
        "For clarity, if the sentence embedding of a single input has shape `[batch_size, embedding_dim]`, then the classification input has shape:\n",
        "\n",
        "*     **Concatenation**: `[batch_size, 2 * embedding_dim]`\n",
        "\n",
        "*     **Sum**: `[batch_size, embedding_dim]`\n",
        "\n",
        "*     **Mean**: `[batch_size, embedding_dim]`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "Qodi5wtE16yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import concatenate, add, average, dot\n",
        "def Classifier(embed_model, model_type, type_merge, cosine_similarity):\n",
        "  input_c = Input(shape=(max_seq_length))\n",
        "  input_e = Input(shape=(max_seq_length))\n",
        "  embedding_c = embed_model(input_c)\n",
        "  embedding_e = embed_model(input_e)\n",
        "\n",
        "  if model_type == \"firstModel\":\n",
        "    embedding_c = embedding_c[1]\n",
        "    embedding_e = embedding_e[1]\n",
        "\n",
        "  if type_merge == \"concat\":\n",
        "      class_input = concatenate([embedding_c, embedding_e])\n",
        "  elif type_merge == \"sum\":\n",
        "      class_input = add([embedding_c, embedding_e])\n",
        "  elif type_merge == \"mean\":\n",
        "      class_input = average([embedding_c, embedding_e])\n",
        "\n",
        "  if cosine_similarity:\n",
        "      cos_sim = dot([embedding_c, embedding_e], axes=1, normalize=True)\n",
        "      class_input = concatenate([class_input, cos_sim])\n",
        "\n",
        "  x = Dense(32, activation=\"relu\")(class_input)\n",
        "  output = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  return Model([input_c, input_e], output, name=\"Classifier\")"
      ],
      "metadata": {
        "id": "fZjsHxzhd4N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.0001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "embed_model = firstModel(embedding_vector_length, 64)\n",
        "base_model = Classifier(embed_model, \"firstModel\", \"mean\", cosine_similarity=True)\n",
        "base_model.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "history = base_model.fit(x=[x_train[0], x_train[1]], y=y_train, \n",
        "                         validation_data=([x_val[0], x_val[1]], y_val), \n",
        "                         epochs=60, batch_size=1024)"
      ],
      "metadata": {
        "id": "GF7mMXyMfeMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = base_model.predict([x_train[0], x_train[1]])\n",
        "print(classification_report(y_train, np.round(predictions)))"
      ],
      "metadata": {
        "id": "3U2xPTPH3HJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = base_model.predict([x_val[0], x_val[1]])\n",
        "print(classification_report(y_val, np.round(predictions)))"
      ],
      "metadata": {
        "id": "j4L2cshu1mYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(tf.keras.Model):\n",
        "  def __init__(self, embedding_vector_length, dim, type_embed, type_merge, \n",
        "               cosine_similarity=False, embed_trainable=True, start_lr=0.001, **kwargs):\n",
        "    super(Classifier, self).__init__(**kwargs)\n",
        "\n",
        "    self.type_embed = type_embed\n",
        "    self.cosine_similarity = cosine_similarity\n",
        "\n",
        "    self.embedding_1 = Embedding(len(tokenizer.vocab.keys())+1, embedding_vector_length, \n",
        "                        input_length=max_seq_length, \n",
        "                        trainable=embed_trainable, \n",
        "                        mask_zero=True)\n",
        "    if type_embed == \"firstModel\":\n",
        "      self.embedding_2 = SimpleRNN(dim,return_state=True)\n",
        "    elif type_embed == \"secondModel\":\n",
        "      self.embedding_2 = SimpleRNN(dim,return_sequences=True)\n",
        "      self.embedding_3 = GlobalAveragePooling1D()\n",
        "    elif type_embed == \"thirdModel\":\n",
        "      self.embedding_2 = Flatten()\n",
        "      self.embedding_3 = Dense(350, input_shape=(embedding_vector_length*max_seq_length,), activation='relu')\n",
        "      self.embedding_4 = Dense(50, activation='relu')\n",
        "      self.embedding_5 = Dense(dim, activation='sigmoid')\n",
        "    elif type_embed == \"fourthModel\":\n",
        "      self.embedding_2 = GlobalAveragePooling1D()\n",
        "\n",
        "    if type_merge == \"concat\":\n",
        "      self.merge = Concatenate()\n",
        "    elif type_merge == \"sum\":\n",
        "      self.merge = Add()\n",
        "    elif type_merge == \"mean\":\n",
        "      self.merge = Average()\n",
        "\n",
        "    self.concatenate = Concatenate()\n",
        "\n",
        "    self.cosine_similarity = Dot(axes=1, normalize=True)\n",
        "\n",
        "    self.dense_1 = Dense(16, activation=\"relu\")\n",
        "    self.activation = Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "  def call(self, input):\n",
        "    claim = input[0]\n",
        "    evidence = input[1]\n",
        "    c_embed = self.embedding_1(claim)\n",
        "    e_embed = self.embedding_1(evidence)\n",
        "\n",
        "    c_embed = self.embedding_2(c_embed)\n",
        "    e_embed = self.embedding_2(e_embed)\n",
        "\n",
        "    if self.type_embed == \"thirdModel\":\n",
        "      c_embed = self.embedding_3(c_embed)\n",
        "      e_embed = self.embedding_3(e_embed)\n",
        "      c_embed = self.embedding_4(c_embed)\n",
        "      e_embed = self.embedding_4(e_embed)\n",
        "      c_embed = self.embedding_5(c_embed)\n",
        "      e_embed = self.embedding_5(e_embed)\n",
        "    elif self.type_embed == \"secondModel\":\n",
        "      c_embed = self.embedding_3(c_embed)\n",
        "      e_embed = self.embedding_3(e_embed)\n",
        "\n",
        "    if self.type_embed == \"firstModel\":\n",
        "      c_embed = c_embed[1]\n",
        "      e_embed = e_embed[1]\n",
        "\n",
        "    class_input = self.merge([c_embed, e_embed])\n",
        "\n",
        "    if self.cosine_similarity:\n",
        "      cos_sim = self.cosine_similarity([c_embed, e_embed])\n",
        "      class_input = self.concatenate([class_input, cos_sim])\n",
        "\n",
        "    x = self.dense_1(class_input)\n",
        "    return self.activation(x)"
      ],
      "metadata": {
        "id": "wSg4lr6Rh1EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "AUeEi0Tb6mKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      0.001,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=0.96,\n",
        "      staircase=True)\n",
        "optim = Adam(learning_rate=lr_schedule)\n",
        "base_model = Classifier(embedding_vector_length, 32, \"firstModel\", \"mean\", \n",
        "                        embed_trainable=True, cosine_similarity=True)\n",
        "base_model.compile(loss='binary_crossentropy', optimizer=optim, \n",
        "                   metrics=['accuracy'])\n",
        "history = base_model.fit(x=[x_train[0], x_train[1]], y=y_train, \n",
        "                         validation_data=([x_val[0], x_val[1]], y_val), \n",
        "                         epochs=100, batch_size=128)"
      ],
      "metadata": {
        "id": "tXnZVgIom7Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KvH6c2z00T4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.evaluate([x_test[0], x_test[1]], y_test)"
      ],
      "metadata": {
        "id": "_cQvMPwscFU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhVg9ZLymOUc"
      },
      "source": [
        "# A simple extension\n",
        "\n",
        "Lastly, we ask you to modify previously defined neural architectures by adding an additional feature to the classification input.\n",
        "\n",
        "We would like to see if some similarity information between the claim to verify and one of its associated evidence might be useful to the classification.\n",
        "\n",
        "Compute the cosine similarity metric between the two sentence embeddings and concatenate the result to the classification input.\n",
        "\n",
        "For clarity, since the cosine similarity of two vectors outputs a scalar value, the classification input shape is modified as follows:\n",
        "\n",
        "*     **Concatenation**: `[batch_size, 2 * embedding_dim + 1]`\n",
        "\n",
        "*     **Sum**: `[batch_size, embedding_dim + 1]`\n",
        "\n",
        "*     **Mean**: `[batch_size, embedding_dim + 1]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd74ULgpnJrc"
      },
      "source": [
        "# Performance evaluation\n",
        "\n",
        "Due to our simplifications, obtained results are not directly compatible with a traditional fact checking method that considers the evidence set as a whole.\n",
        "\n",
        "Thus, we need to consider two types of evaluations.\n",
        "\n",
        "---\n",
        "\n",
        "A. **Multi-input classification evaluation**\n",
        "\n",
        "This type of evaluation is the easiest and concerns computing evaluation metrics, such as accuracy, f1-score, recall and precision, of our pre-processed dataset.\n",
        "\n",
        "In other words, we assess the performance of chosen classifiers.\n",
        "\n",
        "---\n",
        "\n",
        "B. **Claim verification evaluation**\n",
        "\n",
        "However, if we want to give an answer concerning the claim itself, we need to consider the whole evidence set. \n",
        "\n",
        "Intuitively, for a given claim, we consider all its corresponding (claim, evidence) pairs and their corresponding classification outputs. \n",
        "\n",
        "At this point, all we need to do is to compute the final predicted claim label via majority voting.\n",
        "\n",
        "---\n",
        "\n",
        "Example:\n",
        "\n",
        "    Claim: c1\n",
        "    Evidence set: e1, e2, e3\n",
        "    True label: S\n",
        "\n",
        "    Pair outputs:\n",
        "    (c1, e1) -> S (supports)\n",
        "    (c1, e2) -> S (supports)\n",
        "    (c1, e3) -> R (refutes)\n",
        "\n",
        "    Majority voting:\n",
        "    S -> 2 votes\n",
        "    R -> 1 vote\n",
        "\n",
        "    Final label:\n",
        "    c1 -> S\n",
        "\n",
        "Lastly, we have to compute classification metrics just like before.\n",
        "\n",
        "Shortly speaking, implement both strategies for your classification metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4LJ2yPxsUOV"
      },
      "source": [
        "# Tips and Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf80UVRNrXve"
      },
      "source": [
        "## 8.1 Extensions are welcome!\n",
        "\n",
        "Is this task too easy for you? Are you curious to try out things you have seen during lectures (e.g. attention)? Feel free to try everything you want!\n",
        "\n",
        "**Don't forget to try neural baselines first!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COXeCXdYsBEf"
      },
      "source": [
        "## 8.2 Comments and documentation\n",
        "\n",
        "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejv6SDE8xc4_"
      },
      "source": [
        "## 8.3 Organization\n",
        "\n",
        "We suggest you to divide your work into sections. This allows you to build clean and modular code, as well as easy to read and to debug.\n",
        "\n",
        "A possible schema:\n",
        "\n",
        "*   Dataset pre-processing\n",
        "*   Dataset conversion\n",
        "*   Model definition\n",
        "*   Training\n",
        "*   Evaluation\n",
        "*   Comments/Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19QWjgGzIKOq"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Which are the evaluation criteria on which we'll judge you and your work?\n",
        "\n",
        "1. Pre-processing: whether you have done some pre-processing or not.\n",
        "2. Sentence embedding: you should implement all required strategies (with an example and working code for each). That is, we, as evaluators, should be able to test all strategies without writing down new code.\n",
        "3. Multiple inputs merging strategies: you should implement all required strategies (with an example and working code for each).\n",
        "4. Similarity extension: you should implement the cosine similarity extension (with an example and working code).\n",
        "5. Voting strategy: you should implement the majority voting strategy and provide results.\n",
        "6. Report: when submitting your notebook, you should also attach a small summary report that describes what you have done (provide motivations as well for abitrary steps. For instance, \"We've applied L2 regularization since the model was overfitting\".\n",
        "\n",
        "Extras (possible extra points):\n",
        "\n",
        "1. Any well defined extension is welcome!\n",
        "2. Well organized and commented code is as important as any other criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DR70uh7pabo"
      },
      "source": [
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "*Note*: We highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information before contacting us!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc0gNWU2pgKQ"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Can I do something text pre-processing?\n",
        "\n",
        "**Answer:** You have to! If you check text data, the majority of sentences need some cleaning.\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: The model architecture schema is not so clear, are we doing end-to-end training?\n",
        "\n",
        "**Answer**: Exactly! All models can be thought as:\n",
        "\n",
        "1. Input\n",
        "2. (word) Embedding\n",
        "3. Sentence embedding\n",
        "4. Multiple inputs merging\n",
        "5. Classification\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Can I extend models by adding more layers?\n",
        "\n",
        "**Answer**: Feel free to define model architectures as you wish, but remember satisfy our requirements. This assignment should not be thought as a competition to achieve the best performing model: fancy students that want to show off but miss required assignment objectives will be punished!!\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: I'm struggling with the implementation. Can you help me?\n",
        "\n",
        "**Answer**: Yes sure! Contact us and describe your issue. If you are looking for a particular type of operation, you can easily check the documentation of the deep learning framework you are using (google is your friend).\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Can I try other encoding strategies or neural architectures?\n",
        "\n",
        "**Answer:** Absolutely! Remember to try out recommended neural baselines first and only then proceed with your extensions.\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Do we have to test all possible sentence embedding and input merging combinations?\n",
        "\n",
        "**Answer**: Absolutely no! Feel free to pick one sentence embedding strategy and try all possible input merging strategies with it! For instance, pick the best performing sentence embedding method and proceed with next steps (extras included). Please, note that you still have to implement all mentioned strategies!\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: I'm hitting out of memory error when training my models, do you have any suggestions?\n",
        "\n",
        "**Answer**: Here are some common workarounds:\n",
        "\n",
        "1. Try decreasing the mini-batch size\n",
        "2. Try applying a different padding strategy (if you are applying padding): e.g. use quantiles instead of maximum sequence length\n",
        "3. Check the efficiency of your custom code implementation (if any)\n",
        "4. Try to define same length mini-batches to avoid padding (**It should not be necessary here!**)\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: I'm hitting CUDNN_STATUS_BAD_PARAM error! What I'm doing wrong?\n",
        "\n",
        "**Answer**: This error is a little bit tricky since the stack trace is not meaningful at all! This error occurs when the RNN is fed with a sequence of all 0s and pad masking is enabled (e.g. from the embedding layer). Please, check your conversion step, since there might be an error that leads to the encoding of a sentence to all 0s.\n",
        "\n",
        "---"
      ]
    }
  ]
}